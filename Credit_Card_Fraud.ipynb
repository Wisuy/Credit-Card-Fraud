{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "mlg_ulb_creditcardfraud_path = kagglehub.dataset_download('mlg-ulb/creditcardfraud')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "WcesfLXd33oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P5W_kiG31-6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_path = os.path.join(mlg_ulb_creditcardfraud_path, 'creditcard.csv')\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "df['TransactionDensity'] = df['Amount'] / (df['Time'] + 1)\n",
        "\n",
        "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
        "\n",
        "features_to_scale_names = ['Amount', 'TransactionDensity']\n",
        "pca_features = [f'V{i}' for i in range(1, 29)]\n",
        "\n",
        "X = df[features_to_scale_names + pca_features].copy()\n",
        "y = df['Class']\n",
        "\n",
        "print(f\"Full dataset ready for splitting. Total records: {len(df)}\")\n",
        "print(f\"Fraud ratio: {y.mean():.4%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***UNDERSAMPLING***\n"
      ],
      "metadata": {
        "id": "kHf-0JiCqF_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train[features_to_scale_names])\n",
        "X_train[features_to_scale_names] = scaler.transform(X_train[features_to_scale_names])\n",
        "X_test[features_to_scale_names] = scaler.transform(X_test[features_to_scale_names])\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"Training set size (Balanced): {len(X_train)}\")\n",
        "print(f\"Test set size (Real World): {len(X_test)}\")"
      ],
      "metadata": {
        "id": "afKTKYJu4S3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SMOTE***"
      ],
      "metadata": {
        "id": "ZAbS0gKtqBd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train[features_to_scale_names])\n",
        "X_train[features_to_scale_names] = scaler.transform(X_train[features_to_scale_names])\n",
        "X_test[features_to_scale_names] = scaler.transform(X_test[features_to_scale_names])\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "X_train, y_train = X_train_smote, y_train_smote\n",
        "\n",
        "print(f\"Original Training Fraud Count: {y_test.value_counts()[1]}\")\n",
        "print(f\"New Training Set Size (SMOTE): {len(X_train)}\")\n",
        "print(f\"New Fraud Count (SMOTE): {sum(y_train == 1)}\")"
      ],
      "metadata": {
        "id": "xq4zVetCpvL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "iforest = IsolationForest(contamination=0.01, random_state=42)\n",
        "X_full_scaled = pd.concat([X_train, X_test], axis=0)\n",
        "X_full_scaled = X_full_scaled.sort_index()\n",
        "\n",
        "df['isOutlier'] = iforest.fit_predict(X_full_scaled)\n",
        "\n",
        "print(\"Total Anomaly Counts:\")\n",
        "print(df['isOutlier'].value_counts())"
      ],
      "metadata": {
        "id": "5D2IDCip4jxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import recall_score, average_precision_score\n",
        "import pandas as pd\n",
        "\n",
        "# List to store results for graphing later\n",
        "performance_data = []\n",
        "\n",
        "print(f\"{'Model':<25} | {'Recall':<8} | {'AUPRC':<8} | {'Train(s)':<8} | {'Inf(ms)':<8}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Helper function to avoid repeating code for every model\n",
        "def record_performance(name, model, X_train, y_train, X_test, y_test):\n",
        "    # 1. Measure Training Time\n",
        "    start_train = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_train\n",
        "\n",
        "    # 2. Measure Inference Time & Get Predictions\n",
        "    start_inf = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Get scores for AUPRC (Probabilities or Decision Function)\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_scores = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_scores = model.decision_function(X_test)\n",
        "\n",
        "    inf_time = (time.time() - start_inf)\n",
        "    inf_time_ms = (inf_time / len(X_test)) * 1000 # Time per sample in ms\n",
        "\n",
        "    # 3. Calculate Metrics\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    auprc = average_precision_score(y_test, y_scores)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{name:<25} | {rec:.4f}   | {auprc:.4f}   | {train_time:.3f}    | {inf_time_ms:.4f}\")\n",
        "\n",
        "    # Store for graphing\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Recall\": rec,\n",
        "        \"AUPRC\": auprc,\n",
        "        \"Train Time (s)\": train_time,\n",
        "        \"Inference Time (ms)\": inf_time_ms\n",
        "    }\n",
        "\n",
        "# --- Execute Models ---\n",
        "performance_data.append(record_performance(\"Logistic Regression\", LogisticRegression(solver='liblinear', random_state=42), X_train, y_train, X_test, y_test))\n",
        "performance_data.append(record_performance(\"Decision Tree\", DecisionTreeClassifier(random_state=42), X_train, y_train, X_test, y_test))\n",
        "performance_data.append(record_performance(\"Random Forest\", RandomForestClassifier(n_estimators=100, random_state=42), X_train, y_train, X_test, y_test))\n",
        "performance_data.append(record_performance(\"Gradient Boosting\", GradientBoostingClassifier(n_estimators=100, random_state=42), X_train, y_train, X_test, y_test))\n",
        "performance_data.append(record_performance(\"Linear SVC\", LinearSVC(C=1, random_state=42, dual=False), X_train, y_train, X_test, y_test))"
      ],
      "metadata": {
        "id": "y8nhLs3h4QiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import recall_score, average_precision_score\n",
        "\n",
        "# Define Model\n",
        "nn_model = Sequential()\n",
        "nn_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "nn_model.add(Dense(32, activation='relu'))\n",
        "nn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile\n",
        "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Measure Training Time\n",
        "start_train = time.time()\n",
        "nn_model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=0)\n",
        "train_time_nn = time.time() - start_train\n",
        "\n",
        "# Measure Inference Time & Get Predictions\n",
        "start_inf = time.time()\n",
        "y_probs_nn = nn_model.predict(X_test, verbose=0).flatten()\n",
        "y_pred_nn_binary = (y_probs_nn > 0.5).astype('int32')\n",
        "inf_time_nn = time.time() - start_inf\n",
        "inf_time_ms_nn = (inf_time_nn / len(X_test)) * 1000\n",
        "\n",
        "# Calculate Metrics\n",
        "rec_nn = recall_score(y_test, y_pred_nn_binary)\n",
        "auprc_nn = average_precision_score(y_test, y_probs_nn)\n",
        "\n",
        "# Print results\n",
        "print(f\"{'Neural Network':<25} | {rec_nn:.4f}   | {auprc_nn:.4f}   | {train_time_nn:.3f}    | {inf_time_ms_nn:.4f}\")\n",
        "\n",
        "# Store in the same list as your other models\n",
        "performance_data.append({\n",
        "    \"Model\": \"Neural Network\",\n",
        "    \"Recall\": rec_nn,\n",
        "    \"AUPRC\": auprc_nn,\n",
        "    \"Train Time (s)\": train_time_nn,\n",
        "    \"Inference Time (ms)\": inf_time_ms_nn\n",
        "})"
      ],
      "metadata": {
        "id": "_VB3yX2M4n-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# param_grid = {'n_estimators': [10, 50, 100], 'max_depth': [5, 10, 15]}\n",
        "\n",
        "# print(\"GridSearchCV\")\n",
        "# grid_search = GridSearchCV(\n",
        "#     RandomForestClassifier(random_state=42),\n",
        "#     param_grid,\n",
        "#     cv=5,\n",
        "#     scoring='recall',\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# print(\"Grid Search Complete.\")\n",
        "# print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "# print(\"Best Cross-Validation Recall Score:\", f\"{grid_search.best_score_:.4f}\")\n",
        "\n",
        "# best_rf_model = grid_search.best_estimator_\n",
        "# y_pred_best_rf = best_rf_model.predict(X_test)\n",
        "# print(\"Test Set Recall Score (using Best Model):\", f\"{recall_score(y_test, y_pred_best_rf):.4f}\")"
      ],
      "metadata": {
        "id": "cq6bJ84v4xWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Convert the collected data into a DataFrame\n",
        "df_results = pd.DataFrame(performance_data).sort_values(by='AUPRC', ascending=False)\n",
        "\n",
        "# Create the Visualization with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# Performance Metrics (AUPRC and Recall)\n",
        "df_results.plot(x='Model', y=['AUPRC', 'Recall'], kind='bar', ax=ax1, width=0.8, color=['#4C72B0', '#55A868'])\n",
        "ax1.set_title('Detection Performance (Higher is Better)', fontsize=14, pad=15)\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "ax1.tick_params(axis='x', labelrotation=45)\n",
        "\n",
        "# Efficiency (Training Time)\n",
        "sns.barplot(data=df_results, x='Model', y='Train Time (s)', ax=ax2, palette='magma')\n",
        "ax2.set_title('Training Time (Lower is Better)', fontsize=14, pad=15)\n",
        "ax2.set_ylabel('Seconds')\n",
        "ax2.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "ax2.tick_params(axis='x', labelrotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bPiccWhjeIF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2g-Y4NVeIvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}