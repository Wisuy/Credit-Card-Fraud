{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "mlg_ulb_creditcardfraud_path = kagglehub.dataset_download('mlg-ulb/creditcardfraud')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcesfLXd33oi",
        "outputId": "2e2e7691-0147-4ca2-9b25-f53d28e990e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'creditcardfraud' dataset.\n",
            "Data source import complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P5W_kiG31-6",
        "outputId": "b2cdf1c6-39d9-4aba-8693-f3a43d5960f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New sampled DataFrame size: 71201\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data_path = os.path.join(mlg_ulb_creditcardfraud_path, 'creditcard.csv')\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Calculate the engineered feature\n",
        "df['TransactionDensity'] = df['Amount'] / (df['Time'] + 1)\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
        "\n",
        "# Reducing the size of the dataset\n",
        "total_records = len(df)\n",
        "target_sample_size = int(total_records * 0.25)\n",
        "df_majority = df[df['Class'] == 0]\n",
        "df_minority = df[df['Class'] == 1]\n",
        "minority_size = len(df_minority)\n",
        "majority_size_needed = target_sample_size - minority_size\n",
        "\n",
        "if majority_size_needed <= 0:\n",
        "    df_sampled = df_minority.copy()\n",
        "else:\n",
        "    df_majority_undersampled = df_majority.sample(\n",
        "        n=majority_size_needed,\n",
        "        random_state=42\n",
        "    )\n",
        "    df_sampled = pd.concat([df_majority_undersampled, df_minority]).sample(frac=1, random_state=42)\n",
        "\n",
        "df = df_sampled\n",
        "\n",
        "print(f\"New sampled DataFrame size: {len(df)}\")\n",
        "\n",
        "\n",
        "# Define Features and Target\n",
        "features_to_scale_names = ['Amount', 'TransactionDensity']\n",
        "pca_features = [f'V{i}' for i in range(1, 29)]\n",
        "\n",
        "# X is the full feature matrix\n",
        "X = df[features_to_scale_names + pca_features].copy()\n",
        "\n",
        "# y is the target variable\n",
        "y = df['Class']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split the data first\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train[features_to_scale_names])\n",
        "\n",
        "X_train[features_to_scale_names] = scaler.transform(X_train[features_to_scale_names])\n",
        "X_test[features_to_scale_names] = scaler.transform(X_test[features_to_scale_names])"
      ],
      "metadata": {
        "id": "afKTKYJu4S3q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "iforest = IsolationForest(contamination=0.01, random_state=42)\n",
        "# Re-create the full scaled X for Isolation Forest fit (preserving index order)\n",
        "X_full_scaled = pd.concat([X_train, X_test], axis=0)\n",
        "X_full_scaled = X_full_scaled.sort_index()\n",
        "\n",
        "df['isOutlier'] = iforest.fit_predict(X_full_scaled)\n",
        "\n",
        "print(\"Total Anomaly Counts:\")\n",
        "print(df['isOutlier'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D2IDCip4jxZ",
        "outputId": "29c5fb99-04ab-4471-d00c-7b8d893246df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Anomaly Counts:\n",
            "isOutlier\n",
            " 1    70489\n",
            "-1      712\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "print(\"Recall Scores\\n\")\n",
        "\n",
        "# --- Logistic Regression ---\n",
        "lr_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "print(f\"Logistic Regression Recall: {recall_score(y_test, y_pred_lr):.4f}\")\n",
        "\n",
        "# --- Decision Trees ---\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "print(f\"Decision Tree Recall: {recall_score(y_test, y_pred_dt):.4f}\")\n",
        "\n",
        "# --- Random Forest ---\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(f\"Random Forest Recall: {recall_score(y_test, y_pred_rf):.4f}\")\n",
        "\n",
        "# --- Gradient Boosting ---\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "print(f\"Gradient Boosting Recall: {recall_score(y_test, y_pred_gb):.4f}\")\n",
        "\n",
        "# --- Linear SVC (Optimized Replacement for SVC) ---\n",
        "svm_model = LinearSVC(C=1, random_state=42, dual=False)\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "print(f\"Linear SVC Recall: {recall_score(y_test, y_pred_svm):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8nhLs3h4QiR",
        "outputId": "6228dab9-a0ea-4a79-8714-4fa32c5db70a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall Scores\n",
            "\n",
            "Logistic Regression Recall: 0.8041\n",
            "Decision Tree Recall: 0.8108\n",
            "Random Forest Recall: 0.8311\n",
            "Gradient Boosting Recall: 0.8176\n",
            "Linear SVC Recall: 0.8041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Define model\n",
        "nn_model = Sequential()\n",
        "nn_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "nn_model.add(Dense(32, activation='relu'))\n",
        "nn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "nn_model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=0)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_nn = nn_model.predict(X_test, verbose=0)\n",
        "y_pred_nn_binary = (y_pred_nn > 0.5).astype('int32')\n",
        "\n",
        "print(f\"Neural Network Recall: {recall_score(y_test, y_pred_nn_binary):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VB3yX2M4n-L",
        "outputId": "09ca6d84-01d6-4ef0-ca43-4e31136bdc4f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Recall: 0.8311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'n_estimators': [10, 50, 100], 'max_depth': [5, 10, 15]}\n",
        "\n",
        "print(\"GridSearchCV\")\n",
        "grid_search = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='recall',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Grid Search Complete.\")\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Recall Score:\", f\"{grid_search.best_score_:.4f}\")\n",
        "\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "y_pred_best_rf = best_rf_model.predict(X_test)\n",
        "print(\"Test Set Recall Score (using Best Model):\", f\"{recall_score(y_test, y_pred_best_rf):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq6bJ84v4xWq",
        "outputId": "7141b0b4-00d3-4297-e821-3eacc5fdef1f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GridSearchCV\n",
            "Grid Search Complete.\n",
            "Best Hyperparameters: {'max_depth': 15, 'n_estimators': 50}\n",
            "Best Cross-Validation Recall Score: 0.8404\n",
            "Test Set Recall Score (using Best Model): 0.8378\n"
          ]
        }
      ]
    }
  ]
}